\chapter{Background}
\label{cha:background}

This chapter establishes the theoretical foundations necessary for understanding the implementation and evaluation of an LLM-powered customer service assistant for e-commerce applications.

\section{Large Language Models}

Large Language Models (LLMs) are neural network architectures trained to understand and generate natural language responses. These models demonstrate strong performance across general knowledge tasks but require domain-specific context to be effectively applied in specialized applications such as e-commerce customer service.

The fundamental limitation of LLMs for practical applications lies in their training data cutoff and lack of access to real-time, business-specific information.
While these models possess broad linguistic capabilities and general world knowledge, they cannot access current product catalogs, order statuses, or company-specific policies without external information retrieval mechanisms.

\section{Domain-Specific Context in the E-commerce Niche}

This thesis focuses on automating customer service responses that primarily involve retrieving and presenting existing information rather than executing transactional actions.
We designed the automated assistant to handle informational queries by processing and presenting data already stored within the e-commerce system, emphasizing information delivery over system modifications or direct transaction processing.
In particular, three main categories of information are required to effectively respond to customer inquiries:
\begin{itemize}
    \item Order history information for questions regarding order status and contents
    \item Available product information to guide customers through the purchase process and provide relevant product recommendations
    \item E-commerce business rules including accepted payment methods, available delivery options, and courier information
\end{itemize}

\section{Retrieval Augmented Generation}
\label{sec:rag}

Retrieval Augmented Generation (RAG) is a technique that combines information retrieval with language generation to provide more accurate and contextual responses.
The implementation of RAG follows a straightforward process: when a user submits a query, the system first converts the query into a vector representation and searches a vector database for semantically similar content.
The most relevant information is then retrieved and provided as context to the LLM, which generates a response that incorporates this retrieved knowledge alongside its pre-trained capabilities.
This approach allows LLMs to access up-to-date, domain-specific information that wasn't present in their original training data.

\section{Vector Databases}
\label{sec:vecdb}

Vector databases are specialized storage systems designed to efficiently store and retrieve multidimensional vector representations of data; in our case, for example, text embeddings generated by machine learning models.
They enable semantic search capabilities by measuring similarity between vectors, allowing systems to find contextually relevant information even when exact keyword matches don't exist.

\section{Prompt Engineering}
\label{sec:prompt-engineering}

Prompt engineering is the practice of designing and optimizing input prompts to guide LLM behavior and improve response quality.
Effective prompts provide clear instructions, appropriate context, and formatting guidelines that help the model generate more accurate and relevant responses.
In customer service applications, well-crafted prompts can significantly improve the consistency and helpfulness of automated responses by establishing the assistant's role, communication style, and response structure.

\section{Chain-of-Thought and Reasoning Capabilities}
\label{sec:thinking}

Chain-of-Thought (CoT) prompting is a technique that encourages LLMs to break down complex problems into sequential reasoning steps, making their decision-making process more transparent and often more accurate.
By explicitly requesting the model to "think through" a problem step-by-step, CoT prompting can improve performance on tasks requiring logical reasoning, multi-step analysis, or complex decision-making.
Modern LLMs increasingly incorporate built-in reasoning capabilities, often referred to as "thinking" modes, where the model internally processes information through multiple reasoning steps before generating a final response.
These capabilities allow models to perform more thorough analysis of queries, consider multiple aspects of a problem, and provide more thoughtful responses.
The trade-off for enhanced reasoning capabilities typically involves increased computational cost and longer response times, as models spend additional processing cycles on internal reasoning before producing output.
