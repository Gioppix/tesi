\chapter{Introduction}
\label{cha:introduction}

% \section{Abstract}

% Context

E-commerce businesses rely heavily on email marketing to drive revenue growth, but emails have become increasingly saturated and suffer from declining engagement rates.
This has created a pressing need for these businesses to find more effective communication channels to reach their customers.
The messaging platform \textit{WhatsApp} recently released APIs to interact with the platform in a systematic manner, notably the ability to automatically send messages.
This enables e-commerce businesses to leverage WhatsApp as an alternative communication channel that offers significantly higher open and response rates than email while supporting the same marketing capabilities including automated campaigns, customer segmentation, and targeted messaging.
E-commerce businesses can easily establish automated messaging workflows, and customers frequently engage with these communications by responding to the messages.

% Problem

However, this increased engagement presents both opportunities and challenges: e-commerce businesses must now manage a substantially larger volume of customer inquiries, many of which are tedious and do not require specialized knowledge nor complex actions.
WhatsApp has platform-specific limitations that worsen this challenge: it enforces strict restrictions on message responses, particularly limiting businesses from responding to customer messages after a 24-hour window has elapsed since the last customer-initiated message \cite{wa_policy}.

% Solution

To address these issues, this thesis presents the implementation of an LLM-powered assistant for e-commerce customer service.
The assistant autonomously guides customers through the purchase process by suggesting relevant products, answering order inquiries about contents and shipping status, and providing business-specific information.
The system also aids human intervention by prioritizing conversations and categorizing conversations to enable different departments to operate independently and efficiently.
The architecture we chose for the assistant employs a 2-phase approach comprising \textit{Context Gathering}, the first, and \textit{Answer Generation}, the second.
This architectural necessity enables the system to efficiently handle large product catalogs and order histories by first identifying relevant information and then generating appropriate responses based on that context.
Since implementing such a system can be accomplished through various platforms, including no-code solutions, the primary contribution of this thesis lies in the validation.

% Validation

In particular, we focus on developing a systematic process that can automatically and objectively evaluate its performance, in function of the models chosen to complete the tasks.
To do this, we test each model using a set of synthetic chat conversations and evaluate the results using another LLM that acts as a judge (technique known as LLM-as-a-Judge).
We carefully engineered the prompt used for the judge model to match human evaluation standards, enabling consistent and reliable assessment of model performance across all test scenarios.
We validate each of the two phases separately, leveraging their independent operational capability (by providing standardized, synthetically generated context to the \textit{Answer Generation} phase).
Evaluation covers different dimensions for each phase: for the Context Gathering phase, we evaluate the ability of the model to correctly choose what context is needed and how to obtain it; for the Answer Generation Phase, we evaluate the actual message response together with chat categorization and prioritization.
Additionally, both phases are evaluated across three common performance metrics: failure rate, cost, and speed.

Our results show that for the Context Gathering phase, which is a simpler task, smaller models performed best in terms of speed and cost while still achieving very good accuracy.
For the Answer Generation phase, however, smaller models struggled to maintain quality and produced significantly more hallucinations.
Among the larger models, there is no single best choice: organizations must balance between achieving the highest accuracy, minimizing costs, and optimizing response speed based on their specific requirements.

\section{Outline}

We begin by establishing the theoretical foundations in \cref{cha:background}, covering the key technologies and concepts that underpin our assistant implementation.

\Cref{cha:impl} presents the core implementation of the assistant, detailing our 2-phase architecture that addresses computational cost scalability challenges.
The Context Gathering phase (\cref{sec:context-gathering}) analyzes conversation history to determine what contextual information is needed, while the Answer Generation phase (\cref{sec:answer-generation}) uses this context to generate priority levels, mood classification, conversation categories, and actions.
We also discuss output format considerations (\cref{sec:output_format}) across different LLM providers and reliability mechanisms (\cref{sec:reliability}) including retry logic for handling model inconsistencies.

\Cref{cha:validation} presents our systematic evaluation framework using synthetic chat conversations to assess multiple LLM models across major providers.
We employ an LLM-as-a-Judge methodology (\cref{sec:llm-judge}) for automated evaluation, testing both phases independently to measure accuracy, performance, and cost-effectiveness.
The results (\cref{sec:cg-results}, \cref{sec:ag-results}) show that the first phase can be handled by most models, while the more complex second phase requires careful consideration of trade-offs between quality, speed, and cost.

\Cref{cha:related-work} examines how this thesis builds upon and extends existing research in RAG techniques and LLM-as-a-Judge methodologies, with particular focus on their application to customer service automation.

Finally, \cref{cha:conclusion} presents our considerations and potential extensions to enhance the capabilities of the system.
