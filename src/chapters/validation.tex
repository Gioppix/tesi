\chapter{Validation}
\label{cha:validation}

This chapter presents the systematic evaluation of our LLM-powered customer service assistant using multiple models from major providers across both phases of our architecture.
We employ an automated evaluation framework using synthetic chat conversations and LLM-as-a-Judge methodology to assess model performance across accuracy, cost, and speed dimensions.
The evaluation reveals distinct performance characteristics between the simpler Context Gathering phase and the more complex Answer Generation phase, providing insights for production deployment decisions.

\section{Test Dataset}

The test dataset comprises 80 synthetically generated chat conversations.
To ensure fairness and consistency in the evaluation process, all models are tested using this identical dataset.

The size of 80 conversations was selected based on two key considerations: it provides sufficient diversity to encompass a wide range of conversational scenarios and enables thorough testing of model consistency, while simultaneously maintaining manageable computational costs and reasonable testing duration.

\section{Models Under Evaluation}

We tested major LLM providers using their latest available models.
For models that supported both configurations, we evaluated them with built-in Chain of Thoughts (CoT) reasoning enabled and disabled (see \cref{sec:thinking}).
All models were configured with virtually unlimited output token limits, with the primary constraint being the thinking effort or thinking budget for models that support reasoning capabilities.
Where possible, we tuned this thinking parameter to even out cost and response speed.

However, not all available models were included in our evaluation due to cost constraints. Several high-end models were excluded:
\begin{itemize}
    \item \textbf{Anthropic}: Claude Opus 4.0 \cite{anthropic_pricing}
    \item \textbf{OpenAI}: GPT 4.5, o1 Pro, o3 Pro \cite{openai_pricing}
\end{itemize}

\section{Automated Evaluation Using LLM-as-a-Judge}

Given the extensive test dataset combined with multiple models under evaluation, the analysis process generates thousands of responses that require assessment.
Manual evaluation of this volume of data is impractical due to resource constraints and time limitations.

Large Language Models serve as an effective solution for automated evaluation in this context.
The approach requires a single comprehensive prompt engineering phase to establish evaluation criteria that align with human judgment standards, after which the model can consistently apply these criteria across all responses (as J. Gu et al. \cite{llmasajudge} presents).

This methodology is grounded in the principle that evaluation tasks are generally less complex than generation tasks, as judging the quality of an existing response requires analyzing and assessing predetermined criteria rather than synthesizing new information from scratch.
The evaluator model can focus exclusively on determining whether specific quality standards are met, without needing to balance creativity, factual accuracy, and contextual appropriateness simultaneously as required in generation tasks.
Furthermore, this evaluation approach creates an extra layer of review, allowing the LLM to potentially identify errors that might have been missed, even when the same model evaluates its own generated responses.

During the development phase, a reasoning field was incorporated into the evaluation framework to provide transparency regarding the judge model's decision-making process, enabling iterative refinement of the evaluation criteria.

We selected Claude Sonnet 4.0 with built-in CoT enabled as our judge model because it demonstrated the best alignment with manual evaluation standards while maintaining cost-effectiveness.
To validate this choice, we tested alternative models in the judge role and confirmed that the overall performance rankings remained consistent across different judges, with only minor variations comparable to those observed between human evaluators.
Interestingly, we observed that judging models tended to assign slightly higher scores when evaluating responses generated by identical models. This phenomenon does not represent bias, as the judge model had no visibility into which model generated each response under evaluation.
We attribute this alignment to consistent interpretation of instructions between identical models (for instance, how each model defines concepts such as hallucination).
While our evaluation prompts are comprehensive and detailed, some degree of interpretive variation is inherent in any evaluation process.

\section{Phase 1 Evaluation}

The accuracy evaluation for Phase 1 is based on two dimensions:
\begin{itemize}
    \item \textbf{Products Query Accuracy}: This measures whether a query is accurate. A query is considered accurate if it is absent when not needed, or if it is present when needed and correctly captures what the customer requires.
    \item \textbf{Include Orders Accuracy}: This measures the accuracy of including past orders in the context.
\end{itemize}
To monitor performance, another three key metrics are introduced:
\begin{itemize}
    \item \textbf{Failure Rate}: The percentage of cases where models fail to generate valid JSON output for all 3 attempts.
    \item \textbf{Cost}: The computational expense measured in dollars per response, including only model inference calls.
    \item \textbf{Speed}: The response generation time measured in seconds, which includes only the model processing time.
\end{itemize}

\subsection{Results for Phase 1}

The results show that Phase 1 evaluation is a relatively simple task.
All models performed well across both evaluation dimensions, with consistently low failure rates.
The only notable exception was Claude Haiku 3.5, which struggled to produce consistent JSON output.
The remaining models either had access to JSON mode functionality (detailed in \cref{sec:output_format}) or, in the case of more advanced models, were capable enough to complete the test without requiring this feature.
These results indicate that multiple models would be suitable for production use, allowing organizations to prioritize either response speed or cost efficiency based on their specific requirements.

\begin{table}[H]
    \centering
    \footnotesize
    \begin{tabularx}{\textwidth}{Xp{2.5cm}p{2.2cm}p{1.5cm}p{1.8cm}p{2cm}}
        \toprule
        Model                       & Products Query \newline Accuracy (\%) & Include Orders \newline Accuracy (\%) & Failure \newline Rate (\%) & Average \newline Duration (s) & Average \newline Cost (\$ cents) \\
        \midrule
        \rowcolor[gray]{0.9}
        Claude Haiku 3.5            & 88.2                                  & 100.0                                 & 5.0                        & 2.764                         & 0.1006                           \\
        Claude Sonnet 4.0\textbf{*} & 97.5                                  & 98.8                                  & 0.0                        & 9.567                         & 0.6613                           \\
        \rowcolor[gray]{0.9}
        Claude Sonnet 4.0           & 95.0                                  & 98.8                                  & 0.0                        & 2.896                         & 0.2050                           \\
        OpenAI o3-mini\textbf{*}    & 96.2                                  & 100.0                                 & 0.0                        & 4.945                         & 0.1142                           \\
        \rowcolor[gray]{0.9}
        OpenAI o4-mini\textbf{*}    & 95.0                                  & 100.0                                 & 0.0                        & 3.283                         & 0.0518                           \\
        OpenAI o1\textbf{*}         & 97.5                                  & 98.8                                  & 0.0                        & 5.302                         & 2.2228                           \\
        \rowcolor[gray]{0.9}
        OpenAI GPT-4.1              & 98.8                                  & 100.0                                 & 0.0                        & 1.345                         & 0.0946                           \\
        Gemini 2.5 Pro\textbf{*}    & 96.2                                  & 98.8                                  & 0.0                        & 6.859                         & 0.5644                           \\
        \rowcolor[gray]{0.9}
        Gemini 2.5 Flash\textbf{*}  & 95.0                                  & 97.5                                  & 0.0                        & 3.318                         & 0.1224                           \\
        Gemini 2.5 Flash            & 93.8                                  & 97.5                                  & 0.0                        & 0.996                         & 0.0202                           \\
        \rowcolor[gray]{0.9}
        Gemini 2.5 FL\textbf{*}     & 96.2                                  & 97.5                                  & 0.0                        & 1.535                         & 0.0240                           \\
        Gemini 2.5 FL               & 95.0                                  & 100.0                                 & 0.0                        & 0.641                         & 0.0052                           \\
        \bottomrule
    \end{tabularx}

    \begin{itemize}
        \footnotesize
        \item Models marked with \textbf{*} had built-in CoT enabled.
    \end{itemize}
    \caption{Results of different language models in Phase 1 evaluation}
    \label{tab:phase1_results}
\end{table}

\section{Phase 2 Evaluation}

The accuracy evaluation for Phase 2 is based on five key dimensions:
\begin{itemize}
    \item \textbf{Response Rate}: Percentage of times the model chooses to provide a response. A 100\% response rate is not the target, as the test dataset deliberately includes scenarios where the customer service assistant should abstain from responding. Higher response rates are only beneficial when accompanied by high accuracy and low hallucination rates.
    \item \textbf{Message Accuracy}: This evaluates whether the actions appropriately address the customer's needs based on the conversation context. This metric is calculated exclusively for cases where a message is present in the output.
    \item \textbf{Absence of Hallucinations}: This measures whether the model generates factually incorrect or made-up information that is not supported by the provided context. For this evaluation, we only consider the message content and exclude the categories. This metric is calculated exclusively for cases where a message is present in the output.
    \item \textbf{Priority Accuracy}: This measures whether the model correctly assigns priority levels to chats.
    \item \textbf{Categories Accuracy}: This evaluates whether the model accurately categorizes the chats.
\end{itemize}
Additionally, as in Phase 1, we include three performance metrics:
\begin{itemize}
    \item \textbf{Failure Rate}
    \item \textbf{Cost}
    \item \textbf{Speed}
\end{itemize}

\subsection{Results for Phase 2}

Phase 2 presents a significantly more complex evaluation scenario, and the results demonstrate that models begin to encounter greater challenges with this increased complexity.
The evaluation revealed a notable increase in hallucinations across most models.

Claude Sonnet 4.0 without thinking capabilities, despite demonstrating strong reasoning abilities, struggled to maintain consistent JSON output formatting.
Claude Sonnet 4.0 with thinking enabled achieved the highest performance in terms of accuracy and minimized hallucinations.
However, this model exhibited notably conservative behavior, as evidenced by its lowest response rate among all tested models; additionally, this configuration was the slowest performing model and ranked as the second most expensive option.
While this performance might initially appear to be influenced by the fact that Claude Sonnet 4.0 also served as the judge model, we confirmed this result by conducting additional evaluations using a Gemini model as the judge.

An unexpected finding emerged regarding the Gemini model variants: Gemini 2.5 Flash outperformed Gemini 2.5 Pro while operating approximately 4 times faster and 8 times more cost-effectively.
To verify this counterintuitive result, we conducted manual verification of the judge's assessments and confirmed that the smaller model indeed delivered superior performance (refer to \cref{sec:pro_vs_flash} for a detailed example).

Unlike Phase 1, selecting an appropriate model for production deployment in Phase 2 scenarios requires careful consideration of trade-offs between quality, speed, and cost efficiency.

\begin{table}[h]
    \centering
    \footnotesize
    \begin{tabularx}{\textwidth}{Xp{1.1cm}p{1.25cm}p{1.6cm}p{1.3cm}p{1.3cm}p{1.3cm}p{1.3cm}p{1.3cm}}
        \toprule
        Model                            & Response \newline Rate (\%) & Response \newline Accurate (\%) & Hallucinated Info (\%) & Priority Accurate (\%) & Categories Accurate (\%) & Failure Rate (\%) & Average \newline Duration (s) & Average Cost \newline (\$ cents) \\
        \midrule
        \rowcolor[gray]{0.9}
        Claude Haiku 3.5                 & 95.0                        & 76.3                            & 25.0                   & 85.5                   & 100.0                    & 5.0               & 4.812                         & 0.3717                           \\
        Claude Sonnet 4.0\textbf{*}      & 68.8                        & 96.4                            & 7.3                    & 86.2                   & 100.0                    & 0.0               & 23.963                        & 1.9078                           \\
        \rowcolor[gray]{0.9}
        Claude Sonnet 4.0                & 80.0                        & 90.6                            & 14.1                   & 92.4                   & 100.0                    & 1.2               & 11.402                        & 0.8094                           \\
        OpenAI o3-mini\textbf{*}         & 88.8                        & 81.7                            & 12.7                   & 83.8                   & 100.0                    & 0.0               & 5.859                         & 0.3231                           \\
        \rowcolor[gray]{0.9}
        OpenAI o4-mini\textbf{*}         & 83.8                        & 83.6                            & 34.3                   & 81.2                   & 100.0                    & 0.0               & 3.987                         & 0.1536                           \\
        OpenAI o1\textbf{*}              & 82.5                        & 90.9                            & 10.6                   & 82.5                   & 100.0                    & 0.0               & 6.506                         & 5.5855                           \\
        \rowcolor[gray]{0.9}
        OpenAI GPT-4.1                   & 76.3                        & 91.8                            & 13.1                   & 86.2                   & 100.0                    & 0.0               & 2.689                         & 0.4141                           \\
        Gemini 2.5 Pro\textbf{*}         & 75.0                        & 88.3                            & 10.0                   & 83.8                   & 100.0                    & 0.0               & 15.834                        & 1.5860                           \\
        \rowcolor[gray]{0.9}
        Gemini 2.5 Flash\textbf{*}       & 73.8                        & 89.8                            & 8.5                    & 87.5                   & 100.0                    & 0.0               & 4.239                         & 0.2153                           \\
        Gemini 2.5 Flash                 & 76.3                        & 86.9                            & 19.7                   & 85.0                   & 100.0                    & 0.0               & 1.242                         & 0.0752                           \\
        \rowcolor[gray]{0.9}
        Gemini 2.5 \textit{FL}\textbf{*} & 68.8                        & 89.1                            & 9.1                    & 83.8                   & 95.0                     & 0.0               & 2.662                         & 0.0586                           \\
        Gemini 2.5 \textit{FL}           & 72.5                        & 91.4                            & 19.0                   & 86.2                   & 78.8                     & 0.0               & 1.900                         & 0.0425                           \\
        \bottomrule
    \end{tabularx}

    \begin{itemize}
        \footnotesize
        \item \textit{FL} stands for Flash Lite
        \item Models marked with \textbf{*} had built-in CoT enabled.
    \end{itemize}
    \caption{Results of different language models in Phase 2 evaluation}
    \label{tab:phase2_results}
\end{table}
