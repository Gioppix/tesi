\chapter{Validation}
\label{cha:validation}

This chapter presents the systematic evaluation of our LLM-powered customer service assistant using multiple models from major providers across both phases of our architecture.
We begin by describing our test dataset (\cref{sec:dataset}) and the models under evaluation (\cref{sec:models}).
We then detail our automated evaluation framework using LLM-as-a-Judge methodology (\cref{sec:llm-judge}) to assess model performance across various accuracy dimensions.
The evaluation covers the Context Gathering phase (\cref{sec:cg-eval}) with its results (\cref{sec:cg-results}), followed by the Answer Generation phase (\cref{sec:ag-eval}) and its results (\cref{sec:ag-results}).
Our findings reveal distinct performance characteristics between the simpler Context Gathering phase and the more complex Answer Generation phase, providing insights for production deployment decisions as discussed in \cref{sec:hypothetical}.

\section{Test Dataset}
\label{sec:dataset}

The test dataset comprises 80 synthetically generated chat conversations.
To ensure fairness and consistency in the evaluation process, all models are tested using this identical dataset.

The size of 80 conversations was selected based on two key considerations: it provides sufficient diversity to encompass a wide range of conversational scenarios and enables thorough testing of model consistency, while simultaneously maintaining manageable computational costs and reasonable testing duration.

\section{Models Under Evaluation}
\label{sec:models}

We tested major LLM providers using their latest available models.
For models that supported both configurations, we evaluated them with built-in Chain of Thoughts (CoT) reasoning enabled and disabled (see \cref{sec:thinking}).
All models were configured with virtually unlimited output token limits, with the primary constraint being the thinking effort or thinking budget for models that support reasoning capabilities.
Where possible, we tuned this thinking parameter to even out cost and response speed.
However, we excluded certain high-end models from our evaluation due to cost constraints.
Some examples include:
\begin{itemize}
    \item \textbf{Anthropic}: Claude Opus 4.0 \cite{anthropic_pricing}
    \item \textbf{OpenAI}: GPT 4.5, o1 Pro, o3 Pro \cite{openai_pricing}
\end{itemize}

\section{Automated Evaluation Using LLM-as-a-Judge}
\label{sec:llm-judge}

Given the extensive test dataset combined with multiple models under evaluation, the analysis process generates thousands of responses that require assessment.
Manual evaluation of this volume of data is impractical due to resource constraints and time limitations.

Large Language Models serve as an effective solution for automated evaluation in this context.
The approach requires a single comprehensive prompt engineering phase to establish evaluation criteria that align with human judgment standards, after which the model can consistently apply these criteria across all responses (as J. Gu et al. \cite{llmasajudge_survey} presents).

This methodology is grounded in the principle that evaluation tasks are generally less complex than generation tasks, as judging the quality of an existing response requires analyzing and assessing predetermined criteria rather than synthesizing new information from scratch.
The evaluator model can focus exclusively on determining whether specific quality standards are met, without needing to balance creativity, factual accuracy, and contextual appropriateness simultaneously as required in generation tasks.
Furthermore, this evaluation approach creates an extra layer of review, allowing the LLM to potentially identify errors that might have been missed, even when the same model evaluates its own generated responses.

During the development phase, a reasoning field was incorporated into the evaluation framework to provide transparency regarding the decision-making process of the judge model, enabling iterative refinement of the evaluation criteria.

We selected Claude Sonnet 4.0 with built-in CoT enabled as our judge model because it demonstrated the best alignment with manual evaluation standards while maintaining cost-effectiveness.
To validate this choice, we tested alternative models in the judge role and confirmed that the overall performance rankings remained consistent across different judges, with only minor variations comparable to those observed between human evaluators.
Interestingly, we observed that judging models tended to assign slightly higher scores when evaluating responses generated by identical models. This phenomenon does not represent bias, as the judge model had no visibility into which model generated each response under evaluation.
We attribute this alignment to consistent interpretation of instructions between identical models (for instance, how each model defines concepts such as hallucination).
While our evaluation prompts are comprehensive and detailed, some degree of interpretive variation is inherent in any evaluation process.

\section{Context Gathering Phase Evaluation}
\label{sec:cg-eval}

The accuracy evaluation for the Context Gathering phase is based on two dimensions:
\begin{itemize}
    \item \textbf{Products Query Accuracy}: This measures whether a query is accurate. A query is considered accurate if it is absent when not needed, or if it is present when needed and correctly captures what the customer requires.
    \item \textbf{Include Orders Accuracy}: This measures the accuracy of including past orders in the context.
\end{itemize}
To monitor performance, another three key metrics are introduced:
\begin{itemize}
    \item \textbf{Failure Rate}: The percentage of cases where models fail to generate valid JSON output for all 3 attempts.
    \item \textbf{Cost}: The computational expense measured in dollars per response, including only model inference calls.
    \item \textbf{Speed}: The response generation time measured in seconds, which includes only the model processing time.
\end{itemize}

\subsection{Results for the Context Gathering Phase}
\label{sec:cg-results}

The results show that the Context Gathering phase is a relatively simple task.

\begin{table}[H]
    \centering
    \footnotesize
    \begin{tabularx}{\textwidth}{Xp{2.5cm}p{2.2cm}p{1.5cm}p{1.8cm}p{2cm}}
        \toprule
        Model                       & Products Query \newline Accuracy (\%) & Include Orders \newline Accuracy (\%) & Failure \newline Rate (\%) & Average \newline Duration (s) & Average \newline Cost (\$ cents) \\
        \midrule
        \rowcolor[gray]{0.9}
        Claude Haiku 3.5            & 88.2                                  & 100.0                                 & 5.0                        & 2.764                         & 0.1006                           \\
        Claude Sonnet 4.0\textbf{*} & 97.5                                  & 98.8                                  & 0.0                        & 9.567                         & 0.6613                           \\
        \rowcolor[gray]{0.9}
        Claude Sonnet 4.0           & 95.0                                  & 98.8                                  & 0.0                        & 2.896                         & 0.2050                           \\
        OpenAI o3-mini\textbf{*}    & 96.2                                  & 100.0                                 & 0.0                        & 4.945                         & 0.1142                           \\
        \rowcolor[gray]{0.9}
        OpenAI o4-mini\textbf{*}    & 95.0                                  & 100.0                                 & 0.0                        & 3.283                         & 0.0518                           \\
        OpenAI o1\textbf{*}         & 97.5                                  & 98.8                                  & 0.0                        & 5.302                         & 2.2228                           \\
        \rowcolor[gray]{0.9}
        OpenAI GPT-4.1              & 98.8                                  & 100.0                                 & 0.0                        & 1.345                         & 0.0946                           \\
        Gemini 2.5 Pro\textbf{*}    & 96.2                                  & 98.8                                  & 0.0                        & 6.859                         & 0.5644                           \\
        \rowcolor[gray]{0.9}
        Gemini 2.5 Flash\textbf{*}  & 95.0                                  & 97.5                                  & 0.0                        & 3.318                         & 0.1224                           \\
        Gemini 2.5 Flash            & 93.8                                  & 97.5                                  & 0.0                        & 0.996                         & 0.0202                           \\
        \rowcolor[gray]{0.9}
        Gemini 2.5 FL\textbf{*}     & 96.2                                  & 97.5                                  & 0.0                        & 1.535                         & 0.0240                           \\
        Gemini 2.5 FL               & 95.0                                  & 100.0                                 & 0.0                        & 0.641                         & 0.0052                           \\
        \bottomrule
    \end{tabularx}

    \begin{itemize}
        \footnotesize
        \item Models marked with \textbf{*} had built-in CoT enabled.
    \end{itemize}
\end{table}

All models performed well across both evaluation dimensions, with consistently low failure rates.
The only notable exception was Claude Haiku 3.5, which struggled to produce consistent JSON output.
The remaining models either had access to JSON mode functionality (detailed in \cref{sec:output_format}) or, in the case of more advanced models, were capable enough to complete the test without requiring this feature.

These results demonstrate that multiple models are suitable for production deployment, enabling organizations to optimize for either response speed or cost efficiency according to their specific requirements.
The analysis suggests that larger and more expensive models should be excluded from consideration, as they provide no significant performance advantage relative to their increased cost.

The results show that \textit{OpenAI GPT-4.1} achieved the highest accuracy scores while ranking as the fifth most cost-effective and fourth fastest model.
This performance may be attributed to its operation without CoT reasoning.
The smaller Gemini models also demonstrated strong performance, achieving both the fastest response times and lowest costs. However, this efficiency comes with a trade-off of several percentage points in accuracy.
Some E-commerce platforms could benefit from these cost savings.

\section{Answer Generation Phase Evaluation}
\label{sec:ag-eval}

The accuracy evaluation for the Answer Generation phase is based on five key dimensions:
\begin{itemize}
    \item \textbf{Response Rate}: Percentage of times the model chooses to provide a response. A 100\% response rate is not the target, as the test dataset deliberately includes scenarios where the customer service assistant should abstain from responding. Higher response rates are only beneficial when accompanied by high accuracy and low hallucination rates.
    \item \textbf{Message Accuracy}: This evaluates whether the actions appropriately address the customer's needs based on the conversation context. This metric is calculated exclusively for cases where a message is present in the output.
    \item \textbf{Absence of Hallucinations}: This measures whether the model generates factually incorrect or made-up information that is not supported by the provided context. For this evaluation, we only consider the message content and exclude the categories. This metric is calculated exclusively for cases where a message is present in the output.
    \item \textbf{Priority Accuracy}: This measures whether the model correctly assigns priority levels to chats.
    \item \textbf{Categories Accuracy}: This evaluates whether the model accurately categorizes the chats.
\end{itemize}
Additionally, as in the first phase, we include the same three performance metrics: \textit{Failure Rate}, \textit{Cost} and \textit{Speed}.

\subsection{Results for the Answer Generation Phase}
\label{sec:ag-results}

The second phase presents a significantly more complex evaluation scenario, and the results demonstrate that models begin to encounter greater challenges with this increased complexity.
The evaluation revealed a notable increase in hallucinations across most models.

\begin{table}[H]
    \centering
    \footnotesize
    \begin{tabularx}{\textwidth}{Xp{1.1cm}p{1.25cm}p{1.6cm}p{1.3cm}p{1.3cm}p{1.3cm}p{1.3cm}p{1.3cm}}
        \toprule
        Model                            & Response \newline Rate (\%) & Response \newline Accurate (\%) & Hallucinated Info (\%) & Priority Accurate (\%) & Categories Accurate (\%) & Failure Rate (\%) & Average \newline Duration (s) & Average Cost \newline (\$ cents) \\
        \midrule
        \rowcolor[gray]{0.9}
        Claude Haiku 3.5                 & 95.0                        & 76.3                            & 25.0                   & 85.5                   & 100.0                    & 5.0               & 4.812                         & 0.3717                           \\
        Claude Sonnet 4.0\textbf{*}      & 68.8                        & 96.4                            & 7.3                    & 86.2                   & 100.0                    & 0.0               & 23.963                        & 1.9078                           \\
        \rowcolor[gray]{0.9}
        Claude Sonnet 4.0                & 80.0                        & 90.6                            & 14.1                   & 92.4                   & 100.0                    & 1.2               & 11.402                        & 0.8094                           \\
        OpenAI o3-mini\textbf{*}         & 88.8                        & 81.7                            & 12.7                   & 83.8                   & 100.0                    & 0.0               & 5.859                         & 0.3231                           \\
        \rowcolor[gray]{0.9}
        OpenAI o4-mini\textbf{*}         & 83.8                        & 83.6                            & 34.3                   & 81.2                   & 100.0                    & 0.0               & 3.987                         & 0.1536                           \\
        OpenAI o1\textbf{*}              & 82.5                        & 90.9                            & 10.6                   & 82.5                   & 100.0                    & 0.0               & 6.506                         & 5.5855                           \\
        \rowcolor[gray]{0.9}
        OpenAI GPT-4.1                   & 76.3                        & 91.8                            & 13.1                   & 86.2                   & 100.0                    & 0.0               & 2.689                         & 0.4141                           \\
        Gemini 2.5 Pro\textbf{*}         & 75.0                        & 88.3                            & 10.0                   & 83.8                   & 100.0                    & 0.0               & 15.834                        & 1.5860                           \\
        \rowcolor[gray]{0.9}
        Gemini 2.5 Flash\textbf{*}       & 73.8                        & 89.8                            & 8.5                    & 87.5                   & 100.0                    & 0.0               & 4.239                         & 0.2153                           \\
        Gemini 2.5 Flash                 & 76.3                        & 86.9                            & 19.7                   & 85.0                   & 100.0                    & 0.0               & 1.242                         & 0.0752                           \\
        \rowcolor[gray]{0.9}
        Gemini 2.5 \textit{FL}\textbf{*} & 68.8                        & 89.1                            & 9.1                    & 83.8                   & 95.0                     & 0.0               & 2.662                         & 0.0586                           \\
        Gemini 2.5 \textit{FL}           & 72.5                        & 91.4                            & 19.0                   & 86.2                   & 78.8                     & 0.0               & 1.900                         & 0.0425                           \\
        \bottomrule
    \end{tabularx}

    \begin{itemize}
        \footnotesize
        \item \textit{FL} stands for Flash Lite
        \item Models marked with \textbf{*} had built-in CoT enabled.
    \end{itemize}
\end{table}

Claude Sonnet 4.0 without thinking capabilities, despite demonstrating strong reasoning abilities, struggled to maintain consistent JSON output formatting.
Claude Sonnet 4.0 with thinking enabled achieved the highest performance in terms of accuracy and minimized hallucinations.
However, this model exhibited notably conservative behavior, as evidenced by its lowest response rate among all tested models; additionally, this configuration was the slowest performing model and ranked as the second most expensive option.
While this performance might initially appear to be influenced by the fact that Claude Sonnet 4.0 also served as the judge model, we confirmed this result by conducting additional evaluations using a Gemini model as the judge.

An unexpected finding emerged regarding the Gemini model variants: Gemini 2.5 Flash outperformed Gemini 2.5 Pro while operating approximately 4 times faster and 8 times more cost-effectively.
To verify this counterintuitive result, we conducted manual verification of the assessments made by the judge and confirmed that the smaller model indeed delivered superior performance (refer to \cref{sec:pro_vs_flash} for a detailed example).

Unlike in the Context Gathering phase, selecting an appropriate model for production deployment requires careful consideration of trade-offs between quality, cost efficiency and speed.

\section{Hypothetical Application Examples}
\label{sec:hypothetical}

We will now examine two hypothetical scenarios to illustrate practical considerations when selecting models for production deployment.
To quantify the economic impact of model selection, we calculate what percentage of profit is consumed by the AI assistant's operational costs using the following formula.
For this analysis, we assume that the Answer Generation accuracy is the only model characteristic that affects the Conversion Rate.

% Define macros for the variables
\newcommand{\avgMessages}{\overline{N}}
\newcommand{\avgCostCG}{\overline{C}_{cg}}
\newcommand{\avgCostAG}{\overline{C}_{ag}}
\newcommand{\avgProfit}{\overline{P}}
\newcommand{\convRate}{\textit{CR}}
\newcommand{\modelAccuracy}{\alpha}

\[
    \textit{Profit Percentage} = \frac{\avgMessages \times (\avgCostCG + \avgCostAG)}{\avgProfit \times (\convRate \times \modelAccuracy^{\avgMessages})} \times 100\%
\]

where:
\begin{itemize}
    \item $\avgMessages$ = Average message count per conversion
    \item $\avgCostCG$ = Average model cost per message, Context Gathering phase (\$)
    \item $\avgCostAG$ = Average model cost per message, Answer Generation phase (\$)
    \item $\avgProfit$ = Average Order Profit (AOP), excluding the model (\$)
    \item $\convRate$ = Base Conversion Rate (CR) achieved by human assistants
    \item $\modelAccuracy$ = Model accuracy
\end{itemize}

\paragraph*{E-commerce 1}

Our first example considers an e-commerce platform specializing in digital stickers and printable art downloads, with extensive organic advertising that generates thousands of low-intent daily inquiries.
This reflects in the metrics: $\avgMessages = 3$, $\avgProfit = \$6$, and $\convRate = 2\%$.
For the Context Gathering phase, we select \textit{Gemini 2.5 Flash Lite} as it offers the lowest cost, which is essential given the e-commerce platform's constrained profit margins as demonstrated below.
Using our formula, we see that:
\begin{itemize}
    \item \textit{OpenAI o1}
          \begin{itemize}
              \item $\avgCostAG = 5.5855$ cents per response, $\modelAccuracy = 90.9\%$
              \item Total cost \textbf{186.1\%} of profit
          \end{itemize}
    \item \textit{Claude Sonnet 4.0 with CoT}
          \begin{itemize}
              \item $\avgCostAG = 1.9078$ cents per response, $\modelAccuracy = 96.4\%$
              \item Total cost \textbf{53.4\%} of profit
          \end{itemize}
    \item \textit{Gemini 2.5 Flash with CoT}
          \begin{itemize}
              \item $\avgCostAG = 0.2153$ cents per response, $\modelAccuracy = 89.8\%$
              \item Total cost \textbf{7.6\%} of profit
          \end{itemize}
\end{itemize}
Due to the combination of low AOP and low CR, the unit economics make expensive models such as \textit{OpenAI o1} or \textit{Claude Sonnet 4.0} economically unviable.
In this scenario, smaller models with CoT capabilities present a more suitable alternative. \textit{Gemini 2.5 Flash} demonstrated accurate response generation while maintaining one of the lowest hallucination rates among tested models, making it an excellent choice for cost-conscious deployments.

\paragraph*{E-commerce 2}

Our second example is a luxury furniture retailer specializing in high-end home goods with an Average Order Value of approximately \$300 and a highly targeted customer acquisition strategy.
The metrics are: $\avgMessages = 5$, $\avgProfit = \$65.1$, and $\convRate = 15\%$.
For the Context Gathering phase, we select \textit{OpenAI GPT-4.1} ($\avgCostCG = 0.0946$ cents) as it achieved the highest accuracy while maintaining low cost.
Using our formula, we see that:
\begin{itemize}
    \item \textit{Claude Sonnet 4.0 with CoT}
          \begin{itemize}
              \item $\avgCostAG = 1.9078$ cents per response, $\modelAccuracy = 96.4\%$
              \item Total cost \textbf{1.2\%} of profit
          \end{itemize}
    \item \textit{OpenAI GPT-4.1}
          \begin{itemize}
              \item $\avgCostAG = 0.4141$ cents per response, $\modelAccuracy = 91.8\%$
              \item Total cost \textbf{0.4\%} of profit
          \end{itemize}
    \item \textit{OpenAI o1}
          \begin{itemize}
              \item $\avgCostAG = 5.5855$ cents per response, $\modelAccuracy = 90.9\%$
              \item Total cost \textbf{4.7\%} of profit
          \end{itemize}
\end{itemize}
Given the high AOV and the importance of maintaining brand perception through excellent customer service, this business can justify investing in premium models.
\textit{Claude Sonnet 4.0} with CoT enabled would be an excellent choice despite its higher cost, as it demonstrated the highest accuracy and lowest hallucination rates while consuming only 1.1\% of profit.
The conservative response behavior of this model (68.8\% response rate) is actually beneficial in this context, as it is better to escalate complex inquiries to human agents rather than risk providing incorrect information about high-value products.
Alternatively, \textit{OpenAI GPT-4.1} offers a good balance of high accuracy (91.8\% response accuracy) with faster response times and moderate costs, consuming merely 0.4\% of profit, making it suitable for handling the majority of inquiries while maintaining quality standards.
